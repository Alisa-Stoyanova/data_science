{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Praktikum 4: Machine Learning",
   "metadata": {
    "collapsed": false
   },
   "id": "97009eab61d3d0f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aufgabe: Vergleich von Machine Learning Algorithmen für die Bildklassifizierung\n",
    "\n",
    "**Ziel**: \n",
    "\n",
    "Ziel dieses Praktikums ist es, verschiedene Machine Learning Methoden zur Klassifikation von handgeschriebenen Ziffern (MNIST-Datensatz) zu erkunden und zu vergleichen. Durch diese Aufgabe sollt ihr ein tieferes Verständnis für die Stärken, Schwächen und Anwendungsgebiete verschiedener Machine Learning Algorithmen erlangen.\n",
    "\n",
    "**Einführung**:\n",
    "\n",
    "Der MNIST-Datensatz ist eine Sammlung von handgeschriebenen Ziffern, der häufig zum Einstieg in die Bilderkennung und Machine Learning genutzt wird. Er besteht aus 70.000 Bildern, wobei jedes Bild eine der Ziffern von 0 bis 9 darstellt. Diese Aufgabe bietet eine praktische Gelegenheit, grundlegende Techniken des Machine Learnings anzuwenden und zu vergleichen.\n",
    " \n",
    "<img src=\"digits_output.png\" alt=\"image\" width=\"300\" height=\"auto\">\n",
    "\n",
    "**Aufgabenübersicht**:\n",
    "\n",
    "1. **Datenvorverarbeitung**:\n",
    "    - Ladet die erste Version des MNIST-Datensatzes über scikit-learn.\n",
    "    - Teilt den Datensatz in Trainings- und Testsets auf. Das Testset soll 20 % der Daten beinhalten.\n",
    "    - Da es sich bei den Bildern um 28 x 28 Matrizen handelt, die ML Algorithmen jedoch Vektoren als Input erwarten, müsst ihr die Daten entsprechend umformen. Nutzt dafür die Funktion `reshape()`.\n",
    "    - Skaliert die Daten auf den Wertebereich [0, 1]. Nutzt dafür den `MinMaxScaler` von scikit-learn.\n",
    "\n",
    "2. **Klassifikation mit verschiedenen Algorithmen**:\n",
    "    - Trainiert die folgenden Algorithmen:\n",
    "        - Logistic Regression (mit `sklearn`)\n",
    "        - Decision Tree (mit`sklearn`)\n",
    "        - Random Forest (mit `sklearn`)\n",
    "        - Support Vector Machine (mit `sklearn`)\n",
    "        - Neural Network (mit `tensorflow.keras` und dem `Sequential`-Package):\n",
    "            - Verwendet mindestens 2 Hidden Layers (Dense).\n",
    "            - Nutzt die Loss Function `Crossentropy` (Achtung: Es gibt verschiedene Arten! Informiert euch darüber).\n",
    "            - Verwendet die Aktivierungsfunktionen `relu` und `softmax` (Achtet darauf, welche für welche Layer geeignet sind).\n",
    "            - Trainiert das Netz für 10 Epochen.\n",
    "            - Nutzt die Funktion `summary()` um euch die Netzarchitektur ausgeben zu lassen.\n",
    "    - Achtet auf eine sinnvolle Wahl der Hyperparameter. Informiert euch bei Bedarf auf den jeweiligen Dokumentationsseiten der Bibliotheken.\n",
    "    - Evaluiert jedes Modell mit der Metrik `accuracy_score` auf dem Testset (Achtung: Das Vorgehen zur Evaluierung des Neural Networks ist etwas anders!).\n",
    "    - Messt die Trainings- und Inferenzzeiten für die verschiedenen Algorithmen mit der Funktion `time()`.\n",
    "\n",
    "3. **Evaluierung und Vergleich**:\n",
    "    - Vergleicht die Leistung der Modelle in einer Pandas-Tabelle hinsichtlich Trainingszeit und Genauigkeit.\n",
    "    - Beschreibt die Ergebnisse kurz in etwa 5 Sätzen."
   ],
   "id": "d7f535781f311413"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Datenvorverarbeitung",
   "id": "a4ce6244eb8583"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Daten laden\n",
    "\n",
    "Ladet die erste Version des MNIST-Datensatzes über scikit-learn."
   ],
   "id": "6394bac89eeb1af3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:09:34.319680Z",
     "start_time": "2024-07-08T07:09:24.940142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Hier soll euer Code stehen. \n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target"
   ],
   "id": "aaddc29db1fe55e8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train-Test-Split\n",
    "\n",
    "Teilt den Datensatz in Trainings- und Testsets auf. Nutzt die Funktion `train_test_split()` von scikit-learn. Das Testset soll 20 % der Daten beinhalten."
   ],
   "id": "864c1c1e86e57045"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:09:38.177595Z",
     "start_time": "2024-07-08T07:09:34.319680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# x_train = 80 % des Datensatzes, auf den das Modell trainiert wird\n",
    "# y_train = Die Labels für die Daten aus x_train, beim Modelltraining wird durch den Vergleich beider der Fehler berechnet\n",
    "#           und dementsprechend die Parameter so angepasst, dass dieser Fehler minimiert wird (Fehlerminimierung durch u.a. Gradientenabstieg)\n",
    "# x_test = 20 % der restlichen Daten. Am Ende, wenn das Modell meint einen geringen Fehler zu haben, wird der Datensatz (welchen das Modell noch nicht gesehen hat)\n",
    "#           dem Modell übergeben\n",
    "# y_test = Das sind dann die Labels zu x_test. Beim Vergleich wird dann der Fehler berechnet und geguckt, wie gut das Modell wirklich neue Daten klassifizieren kann.\n",
    "#           Dadurch wird overfitting (Überanpassung) verhindert, denn das Modell könnte sonst auch die Daten nur auswendig lernen. Daher gibt es die x_test und y_test\n",
    "#           Datensätze, denn für das Modell sind das sozusagen völlig neue Daten. War der Fehler beim TrainingsSet gering und beim TestSet hoch, dann gab es vermutlich ein\n",
    "#           Overfitting (auswendiglernen)"
   ],
   "id": "63e6c6122e72eea",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Umwandlung in Vektoren\n",
    "\n",
    "Da es sich bei den Bildern um 28 x 28 Matrizen handelt, die ML Algorithmen jedoch Vektoren als Input erwarten, müsst ihr die Daten entsprechend umformen. Nutzt dafür die Funktion `reshape()`.\n"
   ],
   "id": "4b6a7c469b20ef53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:09:38.191376Z",
     "start_time": "2024-07-08T07:09:38.183119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Hier soll euer Code stehen.\n",
    "#Durch as_frame=False fällt die Aufgabe \"Umwandlung in Vektoren\" weg."
   ],
   "id": "bad4d627bfefb869",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Skalierung\n",
    "\n",
    "Skaliert die Daten auf den Wertebereich [0, 1]. Nutzt dafür den `MinMaxScaler` von scikit-learn."
   ],
   "id": "3d6b9474743af83c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:09:38.956813Z",
     "start_time": "2024-07-08T07:09:38.197624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fitte den Scaler auf das Trainingsset und transformiere das Trainingsset\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transformiere das Testset mit dem gefitteten Scaler\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "d2a0e4bea9040ad3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Klassifikation mit verschiedenen Algorithmen\n",
    "   - Trainiert die folgenden Algorithmen:\n",
    "        - Logistic Regression (mit `sklearn`)\n",
    "        - Decision Tree (mit`sklearn`)\n",
    "        - Random Forest (mit `sklearn`)\n",
    "        - Support Vector Machine (mit `sklearn`)\n",
    "        - Neural Network (mit `tensorflow.keras` und dem `Sequential`-Package):\n",
    "            - Verwendet mindestens 2 Hidden Layers (Dense).\n",
    "            - Nutzt die Loss Function `Crossentropy` (Achtung: Es gibt verschiedene Arten! Informiert euch darüber).\n",
    "            - Verwendet die Aktivierungsfunktionen `relu` und `softmax` (Achtet darauf, welche für welche Layer geeignet sind).\n",
    "            - Trainiert das Netz für 10 Epochen.\n",
    "            - Nutzt die Funktion `summary()` um euch die Netzarchitektur ausgeben zu lassen.\n",
    "   - Achtet auf eine sinnvolle Wahl der Hyperparameter. Informiert euch bei Bedarf auf den jeweiligen Dokumentationsseiten der Bibliotheken.\n",
    "   - Messt die Trainings- und Inferenzzeiten für die verschiedenen Algorithmen mit der Funktion `time()`.\n",
    "   - Evaluiert jedes Modell mit der Metrik `accuracy_score` auf dem Testset (Achtung: Das Vorgehen zur Evaluierung des Neural Networks ist etwas anders!)."
   ],
   "id": "a7e875c7f187a4a7"
  },
  {
   "cell_type": "markdown",
   "source": "### Logistic Regression",
   "metadata": {
    "collapsed": false
   },
   "id": "5232eed478b1f2f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:22:49.288561Z",
     "start_time": "2024-07-08T07:22:04.822161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "start_time = time.time()\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "log_reg_train_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "predicted_x = log_reg.predict(X_test_scaled)\n",
    "log_reg_inference_time = time.time() - start_time\n",
    "\n",
    "log_reg_accuracy = accuracy_score(y_test, predicted_x)\n",
    "print(f\"Logistic Regression:\\nTrainTime: {log_reg_train_time}, InferenzTime: {log_reg_inference_time}, Accuracy: {log_reg_accuracy}\")\n"
   ],
   "id": "526e5642ed3a60bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "TrainTime: 44.24384522438049, InferenzTime: 0.14053893089294434, Accuracy: 0.9245714285714286\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decision Tree",
   "id": "14b08b4dbac58583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:25:06.959574Z",
     "start_time": "2024-07-08T07:24:12.788182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree.fit(X_train_scaled, y_train)\n",
    "dec_tree_train_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_dec_tree = dec_tree.predict(X_test_scaled)\n",
    "dec_tree_inference_time = time.time() - start_time\n",
    "\n",
    "dec_tree_accuracy = accuracy_score(y_test, y_pred_dec_tree)\n",
    "print(f\"Decision Tree:\\nAccuracy: {dec_tree_accuracy}, Training Time: {dec_tree_train_time}, Inference Time: {dec_tree_inference_time}\")"
   ],
   "id": "278495ebbd240a05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Accuracy: 0.8780714285714286, Training Time: 52.359572410583496, Inference Time: 0.06300163269042969\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Random Forest",
   "id": "86d636ba7a6a544"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:26:39.101943Z",
     "start_time": "2024-07-08T07:25:14.129932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "rand_forest = RandomForestClassifier()\n",
    "rand_forest.fit(X_train_scaled, y_train)\n",
    "rand_forest_train_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_rand_forest = rand_forest.predict(X_test_scaled)\n",
    "rand_forest_inference_time = time.time() - start_time\n",
    "\n",
    "rand_forest_accuracy = accuracy_score(y_test, y_pred_rand_forest)\n",
    "print(f\"Random Forest;\\nAccuracy: {rand_forest_accuracy}, Training Time: {dec_tree_train_time}, Inference Time: {dec_tree_inference_time}\")"
   ],
   "id": "f19bd21bfc94dbdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest;\n",
      "Accuracy: 0.9697142857142858, Training Time: 52.359572410583496, Inference Time: 0.06300163269042969\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": "### Support Vector Machine",
   "metadata": {
    "collapsed": false
   },
   "id": "8c35d93e90373801"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:47:01.021007Z",
     "start_time": "2024-07-08T07:27:08.005029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "start_time = time.time()\n",
    "svm = SVC()\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "svm_train_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "svm_inference_time = time.time() - start_time\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM:\\nAccuracy: {svm_accuracy}, Training Time: {svm_train_time}, Inference Time: {svm_inference_time}\")\n"
   ],
   "id": "22af731889e7259e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "Accuracy: 0.9786428571428571, Training Time: 680.3569056987762, Inference Time: 512.4025611877441\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "### Neural Network",
   "metadata": {
    "collapsed": false
   },
   "id": "9619ea8d9f6817c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:11:32.509725Z",
     "start_time": "2024-07-08T08:11:30.827056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "import time\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(784,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=128)\n",
    "model_train_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "model_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "model_inference_time = time.time() - start_time\n",
    "\n",
    "print(f\"Neural Network:\\nAccuracy: {model_accuracy}, Training Time: {model_train_time}, Inference Time: {model_inference_time}\")\n"
   ],
   "id": "7873c66b5006f07f",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not find the DLL(s) 'msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading \"Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019\" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dense, Input\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\python\\keras\\__init__.py:22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\__init__.py:45\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2 \u001B[38;5;28;01mas\u001B[39;00m _tf2\n\u001B[0;32m     43\u001B[0m _tf2\u001B[38;5;241m.\u001B[39menable()\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __operators__\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m audio\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m autograph\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m decorator\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dispatch\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mag_ctx\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m control_status_ctx \u001B[38;5;66;03m# line: 34\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimpl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf_convert \u001B[38;5;66;03m# line: 493\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mthreading\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ag_logging\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtf_export\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf_export\n\u001B[0;32m     25\u001B[0m stacks \u001B[38;5;241m=\u001B[39m threading\u001B[38;5;241m.\u001B[39mlocal()\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontext_managers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m control_dependency_on_returns\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m alias_tensors\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograph\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensor_list\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dynamic_list_append\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_array_ops\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontrol_dependency_on_returns\u001B[39m(return_value):\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:45\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config_pb2\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# (b/143110113)\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tensorflow\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tfe\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:34\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplatform\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m self_check\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m \u001B[43mself_check\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreload_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m   \u001B[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001B[39;00m\n\u001B[0;32m     40\u001B[0m   \u001B[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DS\\DS_venv\\Lib\\site-packages\\tensorflow\\python\\platform\\self_check.py:50\u001B[0m, in \u001B[0;36mpreload_check\u001B[1;34m()\u001B[0m\n\u001B[0;32m     48\u001B[0m         missing\u001B[38;5;241m.\u001B[39mappend(dll_name)\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m missing:\n\u001B[1;32m---> 50\u001B[0m       \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     51\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find the DLL(s) \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m. TensorFlow requires that these DLLs \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     52\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbe installed in a directory that is named in your \u001B[39m\u001B[38;5;132;01m%%\u001B[39;00m\u001B[38;5;124mPATH\u001B[39m\u001B[38;5;132;01m%%\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     53\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment variable. You may install these DLLs by downloading \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     54\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMicrosoft C++ Redistributable for Visual Studio 2015, 2017 and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     55\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for your platform from this URL: \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     56\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     57\u001B[0m           \u001B[38;5;241m%\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(missing))\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m   \u001B[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001B[39;00m\n\u001B[0;32m     60\u001B[0m   \u001B[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001B[39;00m\n\u001B[0;32m     61\u001B[0m   \u001B[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001B[39;00m\n\u001B[0;32m     62\u001B[0m   \u001B[38;5;66;03m# SIGILL).\u001B[39;00m\n\u001B[0;32m     63\u001B[0m   \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplatform\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_cpu_feature_guard\n",
      "\u001B[1;31mImportError\u001B[0m: Could not find the DLL(s) 'msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading \"Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019\" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluierung und Vergleich\n",
    "   - Vergleicht die Leistung der Modelle in einer Pandas-Tabelle hinsichtlich Trainingszeit, Inferenzzeit und Test Accuracy.\n",
    "   - Beschreibt die Ergebnisse kurz in etwa 5 Sätzen."
   ],
   "id": "3634918f570ee09b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:12:34.038165Z",
     "start_time": "2024-07-08T08:12:34.009626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Problem mit Neural Network\n",
    "model_accuracy = 0\n",
    "model_train_time = 0\n",
    "model_inference_time = 0\n",
    "\n",
    "data = {\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Support Vector Machine', 'Neural Network'],\n",
    "    'Training Time (s)': [log_reg_train_time, dec_tree_train_time, rand_forest_train_time, svm_train_time, model_train_time],\n",
    "    'Inference Time (ms)': [log_reg_inference_time, dec_tree_inference_time, rand_forest_inference_time, svm_inference_time, model_inference_time],\n",
    "    'Test Accuracy (%)': [log_reg_accuracy, dec_tree_accuracy, rand_forest_accuracy, svm_accuracy, model_accuracy]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ],
   "id": "9fa120b21d361e5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    Model  Training Time (s)  Inference Time (ms)  \\\n",
       "0     Logistic Regression          44.243845             0.140539   \n",
       "1           Decision Tree          52.359572             0.063002   \n",
       "2           Random Forest          83.025008             0.928477   \n",
       "3  Support Vector Machine         680.356906           512.402561   \n",
       "4          Neural Network           0.000000             0.000000   \n",
       "\n",
       "   Test Accuracy (%)  \n",
       "0           0.924571  \n",
       "1           0.878071  \n",
       "2           0.969714  \n",
       "3           0.978643  \n",
       "4           0.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Inference Time (ms)</th>\n",
       "      <th>Test Accuracy (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>44.243845</td>\n",
       "      <td>0.140539</td>\n",
       "      <td>0.924571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>52.359572</td>\n",
       "      <td>0.063002</td>\n",
       "      <td>0.878071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>83.025008</td>\n",
       "      <td>0.928477</td>\n",
       "      <td>0.969714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>680.356906</td>\n",
       "      <td>512.402561</td>\n",
       "      <td>0.978643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Als beste Lösung eignet sich Random Forest, da das Trainieren zwar länger läuft als bei den anderen Modellen (außer SVM), jedoch die Accuracy vie genauer ist und die Inferenzzeit nur knapp eine Sekunde beträgt. SVM hat zwar die höchste Accuracy, dafür aber deutlich merkbare längere Laufzeiten sowohl beim Trainieren, als auch beim Testen. Decision Tree hat mit abstammt die niedrigste Inferenzzeit, dafür aber auch eine eher relativ schwache Accuracy und ist daher nicht wirklich gut für den Datensatz.",
   "id": "69055d14a95f222c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
